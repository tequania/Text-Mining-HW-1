{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "#Here we are importing all the packages that we may need to execute this code\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import (\n",
    "    edit_distance,\n",
    "    jaccard_distance,\n",
    "    )\n",
    "from nltk.util import ngrams\n",
    "from nltk.book import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tequanialake/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tequanialake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/tequanialake/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/tequanialake/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/tequanialake/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/tequanialake/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     /Users/tequanialake/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/tequanialake/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/tequanialake/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download(\"genesis\")\n",
    "nltk.download(\"inaugural\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"nps_chat\")\n",
    "nltk.download(\"webtext\")\n",
    "nltk.download(\"treebank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am opening the file moby.txt as my corpus\n",
    "with open('moby.txt', 'r') as reader:\n",
    "    moby_data = reader.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many tokens in the text?\n",
    "num_of_tokens = len(nltk.word_tokenize(moby_data))\n",
    "num_of_tokens\n",
    "tokens = nltk.word_tokenize(moby_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20754"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many unique tokens in the text?\n",
    "unique_tokens = len(set(nltk.word_tokenize(moby_data)))\n",
    "unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255018"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of tokens after lemmatization of verbs\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "num_of_tokens_lemmatizer = len([lemmatizer.lemmatize(w,'v') for w in nltk.Text(nltk.word_tokenize(moby_data))])\n",
    "num_of_tokens_lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16899"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of unique tokens after lemmatization of verbs\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "num_of_unique_tokens_lemmatizer = len(set([lemmatizer.lemmatize(w,'v') for w in nltk.Text(nltk.word_tokenize(moby_data))]))\n",
    "num_of_unique_tokens_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007450454477723141\n"
     ]
    }
   ],
   "source": [
    "#Percentage of tokens that is \"HISTORY\" or \"history\"\n",
    "token_dict = FreqDist(tokens)\n",
    "print(((token_dict['history'] + token_dict['HISTORY'])*100)/float(len(nltk.word_tokenize(moby_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007450454477723141\n"
     ]
    }
   ],
   "source": [
    "#Percentage of tokens that is history or HISTORY\n",
    "history = [w for w in tokens if  w =='history' or w == 'HISTORY'] \n",
    "print((len(history) / num_of_tokens)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 19204), ('the', 13715), ('.', 7308), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), (';', 4173), ('in', 3908), ('that', 2978)]\n"
     ]
    }
   ],
   "source": [
    "#Top 10 most frequently occuring tokens\n",
    "print(token_dict.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spelling Checker/Recommender\n",
    "#The function spelling_checker takes in a list of words to double check their spelling and returns the correctly spelled word\n",
    "dictionary = words.words()\n",
    "def spelling_checker(input_words):\n",
    "    output = []\n",
    "    for w in input_words:\n",
    "        distances = ((edit_distance(w, x), x) for x in dictionary)\n",
    "        closest_word = min(distances)\n",
    "        output.append(closest_word[1])\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
